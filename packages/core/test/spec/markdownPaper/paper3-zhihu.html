<div class="RichText ztext Post-RichText"><p>如何在<b>知乎</b>中优雅的写数学公式吗？如何在知乎直接贴 markdown 文件？</p>
    <p>我发现一个神网站解决了这个问题： <b><a href="https://link.zhihu.com/?target=http%3A//mdnice.com" class=" external"
                              target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span
            class="invisible">http://</span><span class="visible">mdnice.com</span><span
            class="invisible"></span></a></b></p>
    <figure data-size="normal">
        <noscript><img src="https://pic1.zhimg.com/v2-9f01a7975465a75305b2273cd10a4e98_b.jpg" data-caption=""
                       data-size="normal" data-rawwidth="2842" data-rawheight="1388"
                       class="origin_image zh-lightbox-thumb" width="2842"
                       data-original="https://pic1.zhimg.com/v2-9f01a7975465a75305b2273cd10a4e98_r.jpg"/></noscript>
        <img src="https://pic1.zhimg.com/80/v2-9f01a7975465a75305b2273cd10a4e98_hd.jpg" data-caption=""
             data-size="normal" data-rawwidth="2842" data-rawheight="1388" class="origin_image zh-lightbox-thumb lazy"
             width="2842" data-original="https://pic1.zhimg.com/v2-9f01a7975465a75305b2273cd10a4e98_r.jpg"
             data-actualsrc="https://pic1.zhimg.com/v2-9f01a7975465a75305b2273cd10a4e98_b.jpg" data-lazy-status="ok">
    </figure>
    <figure data-size="normal">
        <noscript><img src="https://pic2.zhimg.com/v2-4a2c6ac5311bf5bd11b9dd2d5a61d789_b.jpg" data-caption=""
                       data-size="normal" data-rawwidth="862" data-rawheight="1174"
                       class="origin_image zh-lightbox-thumb" width="862"
                       data-original="https://pic2.zhimg.com/v2-4a2c6ac5311bf5bd11b9dd2d5a61d789_r.jpg"/></noscript>
        <img src="https://pic2.zhimg.com/80/v2-4a2c6ac5311bf5bd11b9dd2d5a61d789_hd.jpg" data-caption=""
             data-size="normal" data-rawwidth="862" data-rawheight="1174" class="origin_image zh-lightbox-thumb lazy"
             width="862" data-original="https://pic2.zhimg.com/v2-4a2c6ac5311bf5bd11b9dd2d5a61d789_r.jpg"
             data-actualsrc="https://pic2.zhimg.com/v2-4a2c6ac5311bf5bd11b9dd2d5a61d789_b.jpg" data-lazy-status="ok">
    </figure>
    <p>使用 <b><a href="https://link.zhihu.com/?target=http%3A//mdnice.com" class=" external" target="_blank"
                rel="nofollow noreferrer"><span class="invisible">http://</span><span
            class="visible">mdnice.com</span><span class="invisible"></span></a></b>结合 <code>LaTeX</code>
        语法编写数学公式，一键复制到知乎文章，简直太方便了！</p>
    <p>如果对 <code>LaTeX</code>语法不熟悉，可参考知乎问题：<a href="https://www.zhihu.com/question/31298277" class="internal">知乎上的公式是怎么打出来的？</a>。
    </p>
    <p>不如我们来看一下使用<b><a href="https://link.zhihu.com/?target=http%3A//mdnice.com" class=" external" target="_blank"
                       rel="nofollow noreferrer"><span class="invisible">http://</span><span
            class="visible">mdnice.com</span><span class="invisible"></span></a></b>排版的文章，如下所示</p>
    <p>以下来自一个 markdown 文件的一部分： <a
            href="https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Data-Science-Notes/blob/master/0.math/1.CS229/markdown/1.CS229-LinearAlgebra.md"
            class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span
            class="visible">github.com/fengdu78/Dat</span><span class="invisible">a-Science-Notes/blob/master/0.math/1.CS229/markdown/1.CS229-LinearAlgebra.md</span><span
            class="ellipsis"></span></a></p>
    <h2><b>排版效果</b></h2>
    <p>假设<img src="https://www.zhihu.com/equation?tex=f%3A+%5Cmathbb%7BR%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D"
              alt="[公式]" eeimg="1" data-formula="f: \mathbb{R}^{n} \rightarrow \mathbb{R}">是一个函数，它接受<img
            src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bn%7D" alt="[公式]" eeimg="1"
            data-formula="\mathbb{R}^{n}">中的向量并返回实数。那么关于<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"
                                                             eeimg="1" data-formula="x">的<b>黑塞矩阵</b>（也有翻译作海森矩阵），写做：<img
            src="https://www.zhihu.com/equation?tex=%5Cnabla_x+%5E2+f%28A+x%29" alt="[公式]" eeimg="1"
            data-formula="\nabla_x ^2 f(A x)">，或者简单地说，<img src="https://www.zhihu.com/equation?tex=H" alt="[公式]"
                                                           eeimg="1" data-formula="H">是<img
            src="https://www.zhihu.com/equation?tex=n+%5Ctimes+n" alt="[公式]" eeimg="1" data-formula="n \times n">矩阵的偏导数：
    </p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E%7B2%7D+f%28x%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B1%7D%5E%7B2%7D%7D%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B1%7D+%5Cpartial+x_%7B2%7D%7D%7D+%26+%7B%5Ccdots%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B1%7D+%5Cpartial+x_%7Bn%7D%7D%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B2%7D+%5Cpartial+x_%7B1%7D%7D%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B2%7D%5E%7B2%7D%7D%7D+%26+%7B%5Ccdots%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B2%7D+%5Cpartial+x_%7Bn%7D%7D%7D+%5C%5C+%7B%5Cvdots%7D+%26+%7B%5Cvdots%7D+%26+%7B%5Cddots%7D+%26+%7B%5Cvdots%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bn%7D+%5Cpartial+x_%7B1%7D%7D%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bn%7D+%5Cpartial+x_%7B2%7D%7D%7D+%26+%7B%5Ccdots%7D+%26+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bn%7D%5E%7B2%7D%7D%7D%5Cend%7Barray%7D%5Cright%5D+"
             alt="[公式]" eeimg="1"
             data-formula="\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}=\left[\begin{array}{cccc}{\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{1}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2}^{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{n}}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{1}}} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{2}}} &amp; {\cdots} &amp; {\frac{\partial^{2} f(x)}{\partial x_{n}^{2}}}\end{array}\right] ">
    </p>
    <p>换句话说，<img
            src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E%7B2%7D+f%28x%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D"
            alt="[公式]" eeimg="1" data-formula="\nabla_{x}^{2} f(x) \in \mathbb{R}^{n \times n}">，其：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cleft%28%5Cnabla_%7Bx%7D%5E%7B2%7D+f%28x%29%5Cright%29_%7Bi+j%7D%3D%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bi%7D+%5Cpartial+x_%7Bj%7D%7D+"
             alt="[公式]" eeimg="1"
             data-formula="\left(\nabla_{x}^{2} f(x)\right)_{i j}=\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}} ">
    </p>
    <p>注意：黑塞矩阵通常是对称阵：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bi%7D+%5Cpartial+x_%7Bj%7D%7D%3D%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bj%7D+%5Cpartial+x_%7Bi%7D%7D+"
             alt="[公式]" eeimg="1"
             data-formula="\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}}=\frac{\partial^{2} f(x)}{\partial x_{j} \partial x_{i}} ">
    </p>
    <p>与梯度相似，只有当<img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]" eeimg="1" data-formula="f(x)">为实值时才定义黑塞矩阵。
    </p>
    <p>很自然地认为梯度与向量函数的一阶导数的相似，而黑塞矩阵与二阶导数的相似（我们使用的符号也暗示了这种关系）。 这种直觉通常是正确的，但需要记住以下几个注意事项。 首先，对于一个变量<img
            src="https://www.zhihu.com/equation?tex=f%3A+%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D" alt="[公式]"
            eeimg="1" data-formula="f: \mathbb{R} \rightarrow \mathbb{R}">的实值函数，它的基本定义：二阶导数是一阶导数的导数，即：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x%5E%7B2%7D%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x%7D+f%28x%29+"
             alt="[公式]" eeimg="1"
             data-formula="\frac{\partial^{2} f(x)}{\partial x^{2}}=\frac{\partial}{\partial x} \frac{\partial}{\partial x} f(x) ">
    </p>
    <p>然而，对于向量的函数，函数的梯度是一个向量，我们不能取向量的梯度，即:</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+%5Cnabla_%7Bx%7D+f%28x%29%3D%5Cnabla_%7Bx%7D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%7B%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7B1%7D%7D%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7B2%7D%7D%7D+%5C%5C+%7B%5Cvdots%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7Bn%7D%7D%7D%5Cend%7Barray%7D%5Cright%5D+"
             alt="[公式]" eeimg="1"
             data-formula="\nabla_{x} \nabla_{x} f(x)=\nabla_{x}\left[\begin{array}{c}{\frac{\partial f(x)}{\partial x_{1}}} \\ {\frac{\partial f(x)}{\partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(x)}{\partial x_{n}}}\end{array}\right] ">
    </p>
    <p>上面这个表达式没有意义。 因此，黑塞矩阵不是梯度的梯度。 然而，下面这种情况却这几乎是正确的：如果我们看一下梯度<img
            src="https://www.zhihu.com/equation?tex=%5Cleft%28%5Cnabla_%7Bx%7D+f%28x%29%5Cright%29_%7Bi%7D%3D%5Cpartial+f%28x%29+%2F+%5Cpartial+x_%7Bi%7D"
            alt="[公式]" eeimg="1" data-formula="\left(\nabla_{x} f(x)\right)_{i}=\partial f(x) / \partial x_{i}">的第<img
            src="https://www.zhihu.com/equation?tex=i" alt="[公式]" eeimg="1" data-formula="i">个元素，并取关于于<img
            src="https://www.zhihu.com/equation?tex=x" alt="[公式]" eeimg="1" data-formula="x">的梯度我们得到：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7Bi%7D%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bi%7D+%5Cpartial+x_%7B1%7D%7D%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7B2%7D+%5Cpartial+x_%7B2%7D%7D%7D+%5C%5C+%7B%5Cvdots%7D+%5C%5C+%7B%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7Bi%7D+%5Cpartial+x_%7Bn%7D%7D%7D%5Cend%7Barray%7D%5Cright%5D+"
             alt="[公式]" eeimg="1"
             data-formula="\nabla_{x} \frac{\partial f(x)}{\partial x_{i}}=\left[\begin{array}{c}{\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{1}}} \\ {\frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{2}}} \\ {\vdots} \\ {\frac{\partial f(x)}{\partial x_{i} \partial x_{n}}}\end{array}\right] ">
    </p>
    <p>这是黑塞矩阵第<img src="https://www.zhihu.com/equation?tex=i" alt="[公式]" eeimg="1" data-formula="i">行（列）,所以：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E%7B2%7D+f%28x%29%3D%5Cleft%5B%5Cnabla_%7Bx%7D%5Cleft%28%5Cnabla_%7Bx%7D+f%28x%29%5Cright%29_%7B1%7D+%5Cquad+%5Cnabla_%7Bx%7D%5Cleft%28%5Cnabla_%7Bx%7D+f%28x%29%5Cright%29_%7B2%7D+%5Cquad+%5Ccdots+%5Cquad+%5Cnabla_%7Bx%7D%5Cleft%28%5Cnabla_%7Bx%7D+f%28x%29%5Cright%29_%7Bn%7D%5Cright%5D+"
             alt="[公式]" eeimg="1"
             data-formula="\nabla_{x}^{2} f(x)=\left[\nabla_{x}\left(\nabla_{x} f(x)\right)_{1} \quad \nabla_{x}\left(\nabla_{x} f(x)\right)_{2} \quad \cdots \quad \nabla_{x}\left(\nabla_{x} f(x)\right)_{n}\right] ">
    </p>
    <p>简单地说：我们可以说由于：<img
            src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E%7B2%7D+f%28x%29%3D%5Cnabla_%7Bx%7D%5Cleft%28%5Cnabla_%7Bx%7D+f%28x%29%5Cright%29%5E%7BT%7D"
            alt="[公式]" eeimg="1" data-formula="\nabla_{x}^{2} f(x)=\nabla_{x}\left(\nabla_{x} f(x)\right)^{T}">，只要我们理解，这实际上是取<img
            src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+f%28x%29" alt="[公式]" eeimg="1"
            data-formula="\nabla_{x} f(x)">的每个元素的梯度，而不是整个向量的梯度。</p>
    <p>最后，请注意，虽然我们可以对矩阵<img src="https://www.zhihu.com/equation?tex=A%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D" alt="[公式]"
                            eeimg="1" data-formula="A\in \mathbb{R}^{n}">取梯度，但对于这门课，我们只考虑对向量<img
            src="https://www.zhihu.com/equation?tex=x+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D" alt="[公式]" eeimg="1"
            data-formula="x \in \mathbb{R}^{n}">取黑塞矩阵。
        这会方便很多（事实上，我们所做的任何计算都不要求我们找到关于矩阵的黑森方程），因为关于矩阵的黑塞方程就必须对矩阵所有元素求偏导数<img
                src="https://www.zhihu.com/equation?tex=%5Cpartial%5E%7B2%7D+f%28A%29+%2F%5Cleft%28%5Cpartial+A_%7Bi+j%7D+%5Cpartial+A_%7Bk+%5Cell%7D%5Cright%29"
                alt="[公式]" eeimg="1"
                data-formula="\partial^{2} f(A) /\left(\partial A_{i j} \partial A_{k \ell}\right)">，将其表示为矩阵相当麻烦。</p>
    <h3><b>4.3 二次函数和线性函数的梯度和黑塞矩阵</b></h3>
    <p>现在让我们尝试确定几个简单函数的梯度和黑塞矩阵。 应该注意的是，这里给出的所有梯度都是<b>CS229</b>讲义中给出的梯度的特殊情况。</p>
    <p>对于<img src="https://www.zhihu.com/equation?tex=x+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D" alt="[公式]" eeimg="1"
              data-formula="x \in \mathbb{R}^{n}">, 设<img src="https://www.zhihu.com/equation?tex=f%28x%29%3Db%5ETx"
                                                          alt="[公式]" eeimg="1" data-formula="f(x)=b^Tx"> 的某些已知向量<img
            src="https://www.zhihu.com/equation?tex=b+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%7D" alt="[公式]" eeimg="1"
            data-formula="b \in \mathbb{R}^{n}"> ，则：</p>
    <p><img src="https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+b_%7Bi%7D+x_%7Bi%7D+"
            alt="[公式]" eeimg="1" data-formula="f(x)=\sum_{i=1}^{n} b_{i} x_{i} "></p>
    <p>所以：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7Bk%7D%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7Bk%7D%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+b_%7Bi%7D+x_%7Bi%7D%3Db_%7Bk%7D+"
             alt="[公式]" eeimg="1"
             data-formula="\frac{\partial f(x)}{\partial x_{k}}=\frac{\partial}{\partial x_{k}} \sum_{i=1}^{n} b_{i} x_{i}=b_{k} ">
    </p>
    <p>由此我们可以很容易地看出<img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+b%5E%7BT%7D+x%3Db" alt="[公式]" eeimg="1"
                        data-formula="\nabla_{x} b^{T} x=b">。 这应该与单变量微积分中的类似情况进行比较，其中<img
            src="https://www.zhihu.com/equation?tex=%5Cpartial+%2F%28%5Cpartial+x%29+a+x%3Da" alt="[公式]" eeimg="1"
            data-formula="\partial /(\partial x) a x=a">。 现在考虑<img
            src="https://www.zhihu.com/equation?tex=A%5Cin+%5Cmathbb%7BS%7D%5E%7Bn%7D" alt="[公式]" eeimg="1"
            data-formula="A\in \mathbb{S}^{n}">的二次函数<img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dx%5ETAx"
                                                         alt="[公式]" eeimg="1" data-formula="f(x)=x^TAx">。 记住这一点：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+A_%7Bi+j%7D+x_%7Bi%7D+x_%7Bj%7D+"
             alt="[公式]" eeimg="1" data-formula="f(x)=\sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j} "></p>
    <p>为了取偏导数，我们将分别考虑包括<img src="https://www.zhihu.com/equation?tex=x_k" alt="[公式]" eeimg="1" data-formula="x_k">和<img
            src="https://www.zhihu.com/equation?tex=x_2%5Ek" alt="[公式]" eeimg="1" data-formula="x_2^k">因子的项：</p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7Bk%7D%7D+%26%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7Bk%7D%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+A_%7Bi+j%7D+x_%7Bi%7D+x_%7Bj%7D+%5C%5C+%26%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7Bk%7D%7D%5Cleft%5B%5Csum_%7Bi+%5Cneq+k%7D+%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bi+j%7D+x_%7Bi%7D+x_%7Bj%7D%2B%5Csum_%7Bi+%5Cneq+k%7D+A_%7Bi+k%7D+x_%7Bi%7D+x_%7Bk%7D%2B%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bk+j%7D+x_%7Bk%7D+x_%7Bj%7D%2BA_%7Bk+k%7D+x_%7Bk%7D%5E%7B2%7D%5Cright%5D+%5C%5C+%26%3D%5Csum_%7Bi+%5Cneq+k%7D+A_%7Bi+k%7D+x_%7Bi%7D%2B%5Csum_%7Bj+%5Cneq+k%7D+A_%7Bk+j%7D+x_%7Bj%7D%2B2+A_%7Bk+k%7D+x_%7Bk%7D+%5C%5C+%26%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+A_%7Bi+k%7D+x_%7Bi%7D%2B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+A_%7Bk+j%7D+x_%7Bj%7D%3D2+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+A_%7Bk+i%7D+x_%7Bi%7D+%5Cend%7Baligned%7D+"
             alt="[公式]" eeimg="1"
             data-formula="\begin{aligned} \frac{\partial f(x)}{\partial x_{k}} &amp;=\frac{\partial}{\partial x_{k}} \sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} x_{i} x_{j} \\ &amp;=\frac{\partial}{\partial x_{k}}\left[\sum_{i \neq k} \sum_{j \neq k} A_{i j} x_{i} x_{j}+\sum_{i \neq k} A_{i k} x_{i} x_{k}+\sum_{j \neq k} A_{k j} x_{k} x_{j}+A_{k k} x_{k}^{2}\right] \\ &amp;=\sum_{i \neq k} A_{i k} x_{i}+\sum_{j \neq k} A_{k j} x_{j}+2 A_{k k} x_{k} \\ &amp;=\sum_{i=1}^{n} A_{i k} x_{i}+\sum_{j=1}^{n} A_{k j} x_{j}=2 \sum_{i=1}^{n} A_{k i} x_{i} \end{aligned} ">
    </p>
    <p>最后一个等式，是因为<img src="https://www.zhihu.com/equation?tex=A" alt="[公式]" eeimg="1" data-formula="A">是对称的（我们可以安全地假设，因为它以二次形式出现）。
        注意，<img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+f%28x%29" alt="[公式]" eeimg="1"
                data-formula="\nabla_{x} f(x)">的第<img src="https://www.zhihu.com/equation?tex=k" alt="[公式]" eeimg="1"
                                                      data-formula="k">个元素是<img
                src="https://www.zhihu.com/equation?tex=A" alt="[公式]" eeimg="1" data-formula="A">和<img
                src="https://www.zhihu.com/equation?tex=x" alt="[公式]" eeimg="1" data-formula="x">的第<img
                src="https://www.zhihu.com/equation?tex=k" alt="[公式]" eeimg="1" data-formula="k">行的内积。 因此，<img
                src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+x%5E%7BT%7D+A+x%3D2+A+x" alt="[公式]" eeimg="1"
                data-formula="\nabla_{x} x^{T} A x=2 A x">。 同样，这应该提醒你单变量微积分中的类似事实，即<img
                src="https://www.zhihu.com/equation?tex=%5Cpartial+%2F%28%5Cpartial+x%29+a+x%5E%7B2%7D%3D2+a+x"
                alt="[公式]" eeimg="1" data-formula="\partial /(\partial x) a x^{2}=2 a x">。</p>
    <p>最后，让我们来看看二次函数<img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dx%5ETAx" alt="[公式]" eeimg="1"
                         data-formula="f(x)=x^TAx">黑塞矩阵（显然，线性函数<img src="https://www.zhihu.com/equation?tex=b%5ETx"
                                                                    alt="[公式]" eeimg="1" data-formula="b^Tx">的黑塞矩阵为零）。在这种情况下:
    </p>
    <p>
        <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5E%7B2%7D+f%28x%29%7D%7B%5Cpartial+x_%7Bk%7D+%5Cpartial+x_%7B%5Cell%7D%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7Bk%7D%7D%5Cleft%5B%5Cfrac%7B%5Cpartial+f%28x%29%7D%7B%5Cpartial+x_%7B%5Cell%7D%7D%5Cright%5D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_%7Bk%7D%7D%5Cleft%5B2+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+A_%7B%5Cell+i%7D+x_%7Bi%7D%5Cright%5D%3D2+A_%7B%5Cell+k%7D%3D2+A_%7Bk+%5Cell%7D+"
             alt="[公式]" eeimg="1"
             data-formula="\frac{\partial^{2} f(x)}{\partial x_{k} \partial x_{\ell}}=\frac{\partial}{\partial x_{k}}\left[\frac{\partial f(x)}{\partial x_{\ell}}\right]=\frac{\partial}{\partial x_{k}}\left[2 \sum_{i=1}^{n} A_{\ell i} x_{i}\right]=2 A_{\ell k}=2 A_{k \ell} ">
    </p>
    <p>因此，应该很清楚<img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E2+x%5E%7BT%7D+A+x%3D2+A" alt="[公式]"
                    eeimg="1" data-formula="\nabla_{x}^2 x^{T} A x=2 A">，这应该是完全可以理解的（同样类似于<img
            src="https://www.zhihu.com/equation?tex=%5Cpartial%5E2+%2F%28%5Cpartial+x%5E2%29+a+x%5E%7B2%7D%3D2a"
            alt="[公式]" eeimg="1" data-formula="\partial^2 /(\partial x^2) a x^{2}=2a">的单变量事实）。</p>
    <p>简要概括起来：</p>
    <ul>
        <li><img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+b%5E%7BT%7D+x%3Db" alt="[公式]" eeimg="1"
                 data-formula="\nabla_{x} b^{T} x=b"><br></li>
        <li><img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D+x%5E%7BT%7D+A+x%3D2+A+x" alt="[公式]" eeimg="1"
                 data-formula="\nabla_{x} x^{T} A x=2 A x"> (如果<img src="https://www.zhihu.com/equation?tex=A"
                                                                    alt="[公式]" eeimg="1" data-formula="A">是对称阵)<br></li>
        <li><img src="https://www.zhihu.com/equation?tex=%5Cnabla_%7Bx%7D%5E2+x%5E%7BT%7D+A+x%3D2+A+" alt="[公式]"
                 eeimg="1" data-formula="\nabla_{x}^2 x^{T} A x=2 A "> (如果<img
                src="https://www.zhihu.com/equation?tex=A" alt="[公式]" eeimg="1" data-formula="A">是对称阵)<br></li>
    </ul>
    <hr>
    <figure data-size="normal">
        <noscript><img src="https://pic1.zhimg.com/v2-84f6f6087ca300beffb0930e62184480_b.jpg" data-caption=""
                       data-size="normal" data-rawwidth="720" data-rawheight="419"
                       class="origin_image zh-lightbox-thumb" width="720"
                       data-original="https://pic1.zhimg.com/v2-84f6f6087ca300beffb0930e62184480_r.jpg"/></noscript>
        <img src="https://pic1.zhimg.com/80/v2-84f6f6087ca300beffb0930e62184480_hd.jpg" data-caption=""
             data-size="normal" data-rawwidth="720" data-rawheight="419" class="origin_image zh-lightbox-thumb lazy"
             width="720" data-original="https://pic1.zhimg.com/v2-84f6f6087ca300beffb0930e62184480_r.jpg"
             data-actualsrc="https://pic1.zhimg.com/v2-84f6f6087ca300beffb0930e62184480_b.jpg" data-lazy-status="ok">
    </figure>
    <p></p></div>